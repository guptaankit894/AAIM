{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "trGy7qW1pb7F",
        "qYGQHx_ksP18",
        "CtCvrbtdQARn"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guptaankit894/AAIM/blob/main/google_colab_files/04_Classification_of_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **AAIM Curse_Classification of Data Notebook A**\n",
        " By Dr. Karla Reyes\n",
        "\n"
      ],
      "metadata": {
        "id": "pYjy9jRko0Fs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. ## Logistic Regression class - Diagnosis\n",
        "\n",
        "```\n",
        "Class task #1: Modify the input data, using the data example of the class, choose 5 different features (name them in the output) and 10 different patients, as output print the features values (X and Y), and the probability of diabetes of a new patient.  \n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "trGy7qW1pb7F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5o_hut6oi47"
      },
      "outputs": [],
      "source": [
        "#Suppose we want to diagnose a patient with diabetes based on their age, blood pressure, and body mass index (BMI)\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Define the data (Age, BP, BMI)\n",
        "X = np.array([[25, 120, 25], [45, 140, 30], [55, 160, 35]])\n",
        "y = np.array([0, 1, 1])\n",
        "\n",
        "# Create a logistic regression object\n",
        "logreg = LogisticRegression()\n",
        "\n",
        "# Fit the model\n",
        "logreg.fit(X, y)\n",
        "\n",
        "# Predict the probability of diabetes for a new patient\n",
        "new_patient = np.array([30, 130, 28]).reshape(1, -1) # Reshape to a 2D array\n",
        "probability = logreg.predict_proba(new_patient)[0][1]\n",
        "\n",
        "print(\"Probability of diabetes:\", probability)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. ## Decision tree - classifying patients\n",
        "\n",
        "```\n",
        "Class task #2: Analyze which feature (age, blood pressure, cholesterol, etc..) is the most important in making the classification decision. How could could you determine this experimentally?\n",
        "Hint: use clf.feature_importances_ and interpret the values.\n",
        "```"
      ],
      "metadata": {
        "id": "qYGQHx_ksP18"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Suppose we want to classify patients as either healthy or diseased based on their age, blood pressure, and cholesterol levels, smoker [0=no, 1 =yes], alcohol consume [0=no, 1 =yes], .\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Define the data\n",
        "X = np.array([[25, 110, 180, 0 , 1 , 9], [35, 130, 190, 1 , 0 , 17], [45, 140, 200, 1, 0, 16],[21, 181, 210, 0, 1, 3],[15, 100, 200, 1, 1, 2]])\n",
        "y = np.array([0, 0, 1, 1, 1]) # Healthy = 0, Diseased = 1\n",
        "\n",
        "# Create a decision tree classifier\n",
        "clf = DecisionTreeClassifier(max_depth=2)\n",
        "\n",
        "# Fit the model\n",
        "clf.fit(X, y)\n",
        "\n",
        "# Predict the class for a new patient\n",
        "new_patient = np.array([]).reshape(1,-1) #add mew patient features\n",
        "prediction = clf.predict(new_patient)\n",
        "\n",
        "print(\"Prediction:\", prediction)\n"
      ],
      "metadata": {
        "id": "xM7HW376pvIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. ## Prediction Accuracy by Decision Trees\n",
        "\n",
        "```\n",
        "# Class Task #3: Load Ovarian cancer data \"Data1.csv\". The purpose is to build a tree with training data and apply the test data to the same learned tree and calculate the prediction accuracy.\n",
        "```"
      ],
      "metadata": {
        "id": "CtCvrbtdQARn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the libraries\n",
        "from sklearn import tree\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "_xIeIl6rQNmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This data set contain 216 subjects each of which presented with 100 quantified values (clinical Proteomics). Out of all the subjects, 121 are diagnosed as ovarian cancer and 95 are healthy subjects. Therefore, it is binary classification and the class labels are set as -1 and +1."
      ],
      "metadata": {
        "id": "TUNkaWKscRAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOdKF_0cQYjP",
        "outputId": "e45b599d-378b-48a1-ee04-3bf26df9141c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Reading the data (features and labels)\n",
        "data = pd.read_csv('/content/Data1.csv', header=None)\n",
        "x = data.values[:, :100]\n",
        "y = data.values[:, 100]"
      ],
      "metadata": {
        "id": "cDWGY-_PO4Hj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the data into train and test\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.2, random_state=0)"
      ],
      "metadata": {
        "id": "kT5dhFunQcvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compiling a tree\n",
        "tree_depth = 2\n",
        "clf = tree.DecisionTreeClassifier(criterion='entropy', max_depth=tree_depth , random_state=0)\n",
        "\n",
        "# Learning the compiled tree with training data\n",
        "clf = clf.fit(x_train, y_train)"
      ],
      "metadata": {
        "id": "sbyITk81RvR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  predict the class labels of test data set using \"clf.predict\"\n",
        "y_pred = clf.predict(x_test)\n",
        "\n",
        "# Find the accuracy of the model by comparing the achieved class labels and real labels.\n",
        "test_acc = accuracy_score(y_test, y_pred)\n"
      ],
      "metadata": {
        "id": "Ii7AGg40RylC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Effect of tree depth on the model performance"
      ],
      "metadata": {
        "id": "67j0AhdhR8ma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d_depth = []  # Add a list of different depths, i.e. 1,2,3,....20 to determinate wich depth one has better performance\n",
        "for d in d_depth:\n",
        "    clf = tree.DecisionTreeClassifier(criterion='entropy', max_depth=d , random_state=0)\n",
        "    clf = clf.fit(x_train, y_train)\n",
        "    y_pred = clf.predict(x_test)\n",
        "    test_acc = accuracy_score(y_test, y_pred)\n",
        "    print('Test accuracy with depth of {} is {}'.format(d, test_acc))"
      ],
      "metadata": {
        "id": "5l-G4HAGR5_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. ## Binary Classification with kNN\n",
        "\n",
        "```\n",
        "# Class Task #4: Follow the TODO in the code and write the answers in a word document\n",
        "```"
      ],
      "metadata": {
        "id": "MUBIz4jZTALO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "metadata": {
        "id": "WzgFUUiNcFkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the required libraries.\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn import metrics\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import seaborn as sns\n",
        "sns.set()"
      ],
      "metadata": {
        "id": "A1ATHjCbTJJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " We start with a simple example of two class classification problem. The dataset contains 569 subjects from each 30 features were extracted and labeled as 1 or 0 to present the malignant or benign breast cancer."
      ],
      "metadata": {
        "id": "mmdFr2TgYEkz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the data (features and labels)\n",
        "cancer_breast = load_breast_cancer()\n",
        "x = pd.DataFrame(cancer_breast.data, columns = cancer_breast.feature_names)\n",
        "x = x[['mean area', 'mean compactness']]\n",
        "y = pd.Categorical.from_codes(cancer_breast.target, cancer_breast.target_names)\n",
        "y = pd.get_dummies(y, drop_first=True)"
      ],
      "metadata": {
        "id": "Uw_aui0vX_nE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model should be learned with training data, then, the performance of the learned model should be evaluated over the unseen test data."
      ],
      "metadata": {
        "id": "o2w3Hr-jZMkV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting 25% of the data into train and the rest to the test sets.\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=1)"
      ],
      "metadata": {
        "id": "DX7sTv4vZPGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compiling the KNN model\n",
        "n_neighbor = 3\n",
        "knn = KNeighborsClassifier(n_neighbors = n_neighbor, metric='euclidean')\n",
        "# Training the complied model with training data\n",
        "knn.fit(x_train, y_train)"
      ],
      "metadata": {
        "id": "qUXVD4DyYKUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicting the unseen data and quantify the model performance:\n",
        "y_pred = knn.predict(x_test)\n",
        "print('Prediction Accuracy Score is:',metrics.accuracy_score(y_test,y_pred))"
      ],
      "metadata": {
        "id": "6_vrvAE0ZZzK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Visualization of the KNN Performance:\n",
        "The dimension of the employed data set is 30. Therefore, for simplicity, we use only two features to visualize a 2D scatterplot of the datapoints compared against the predicted ones."
      ],
      "metadata": {
        "id": "cqJ0d_znZngn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scatter plot of the prediction\n",
        "sns.scatterplot(\n",
        "    x='mean area',\n",
        "    y='mean compactness',\n",
        "    hue='benign',\n",
        "    data=x_test.join(y_test, how='outer'))"
      ],
      "metadata": {
        "id": "KaYOFEvYZhKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scatter plot of the real data\n",
        "plt.scatter(\n",
        "    x_test['mean area'],\n",
        "    x_test['mean compactness'],\n",
        "    c=y_pred,\n",
        "    cmap='coolwarm',\n",
        "    alpha=0.8)"
      ],
      "metadata": {
        "id": "mBOMvimWaKCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Changing the value of nearest neighbors and find out its effect!\n",
        "To better understand how the value of nearest neighbor (K) affect the performance and generalization power\n",
        "of the KNN model, we are going to train different KNN models with different K values."
      ],
      "metadata": {
        "id": "DS7l_BJFa8Yx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k_val = [] # Add a list of different NN, i.e. 1,2,3,....20 to determinate wich has better performance\n",
        "\n",
        "for k in k_val:\n",
        "    knn = KNeighborsClassifier(n_neighbors = k, metric='euclidean')\n",
        "    knn.fit(x_train, y_train)\n",
        "    y_pred = knn.predict(x_test)\n",
        "    performance = metrics.accuracy_score(y_test,y_pred)\n",
        "    print('Prediction performance of KNN with K of {} is {}'.format(k, performance))"
      ],
      "metadata": {
        "id": "bxHRg__Xayyl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. ## Binary classification with SVM\n",
        "\n",
        "```\n",
        "# Class Task #5: Follow the TODO in the code and write the answers in a word document\n",
        "```"
      ],
      "metadata": {
        "id": "DtqAQVZQfAVM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing the libraries\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "import seaborn as sns; sns.set()"
      ],
      "metadata": {
        "id": "TNtAgF1yfYF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The purpose here is to define a line in 2D or a manifold in multi-dimensions to divide classes of the data. For instance, consider the following simple case of a 2D data set which are linearly separable."
      ],
      "metadata": {
        "id": "StnzCiRmft6G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/Data0.csv', header=None)\n",
        "X = data.values[:, :2]\n",
        "y = data.values[:, 2]\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=60, cmap='viridis');"
      ],
      "metadata": {
        "id": "BdIp9QmSfcgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this simple example, there are two classes highlighted with different colors. The classes do not have overlap and therefore you can even use a linear equation to separate them from each other.."
      ],
      "metadata": {
        "id": "O6lcz8TAf2Ya"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " However, the problem is that there are unlimited number of such lines that are able to separate the classes. Based on which lines you choose,  a new data point will be either below or above the line and will be assigned to one of the classes. (Look at the point marked by \"X\" in the following plot)."
      ],
      "metadata": {
        "id": "OihGJydAgK4F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xfit = np.linspace(-0.5, 3)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=60, cmap='viridis')\n",
        "\n",
        "# The following script draws a line by following the equation \"Y-y0 = m(X-x0)\"\n",
        "# on the data plane. Please note the value inside each of the parentheses\n",
        "# represent the parameters of 'm' and 'b' of the equation.\n",
        "for m, b in [(-0.67, 2.2), (-1.5,2.5)]:\n",
        "    plt.plot(xfit, m * xfit + b, '-k')\n",
        "\n",
        "plt.plot([2.5],[0],'X',color='red', markeredgewidth=1, markersize=8)\n",
        "\n",
        "plt.xlim(-0.5, 3);\n",
        "\n",
        "# TODO: Try different sepparating lines by modifing m,b\n",
        "# TODO: Was the point \"X\" assigned to the same class when you used different separating line?"
      ],
      "metadata": {
        "id": "3o8CWm5AfmRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It would be possible that the new data point was assigned to a different class based on the line you draw. One way to address such problems is to use two separating lines instead of only one line and maximize the distance between the two separating lines (maximize the margins). In fact, using only one line for separation is like to use two lines with a zero-width of margin. Therefore, we can add a margin between the separating lines and the line that maximizes this margin can be selected as an optimal line in SVM algorithm."
      ],
      "metadata": {
        "id": "Fjydoikugg67"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_blobs # Importing make_blobs from the correct module\n",
        "X, y = make_blobs(n_samples=50, centers=2,random_state=0, cluster_std=0.60)\n",
        "xfit = np.linspace(-1, 3.5)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=60, cmap='viridis')\n",
        "\n",
        "# Comparing to the previous lines, here another parameter \"d\" is added.\n",
        "# The parameter 'd' works as a bias to move the lines up and down.\n",
        "# The following parameters of 'd' inside the parentheses are found experimentally.\n",
        "# What SVM does, is finding the such parameters automatically.\n",
        "\n",
        "for m, b, d in [ (0.5, 1.6, 0.6), (-0.2, 2.9, 0.2)]:\n",
        "    yfit = m * xfit + b\n",
        "    plt.plot(xfit, yfit, '-k')\n",
        "    plt.fill_between(xfit, yfit - d, yfit + d, edgecolor='none',\n",
        "                     color='#AAAAAA', alpha=0.6)\n",
        "\n",
        "plt.xlim(-1, 3.5);\n",
        "\n",
        "# TODO: Change the parameters 'd' inside the parentheses (0.6 and 0.2) and\n",
        "# notice how it would change the margins. Please note the margins should not, optimally,\n",
        "# include any data."
      ],
      "metadata": {
        "id": "74Jhu30CiYMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, instead of finding the line and margin width manually, we are going to train a model by using a linear SVM to find the optimal parameters."
      ],
      "metadata": {
        "id": "_3T4rteejA3G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC # \"Support vector classifier\"\n",
        "model = SVC(kernel='linear', C=1E10)  # Set the linear kernel and a large value of C\n",
        "model.fit(X, y) # Fitting the data set to the model"
      ],
      "metadata": {
        "id": "I3C9Q666jEpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To understand what SVM does, it is better to visualize how it find the decision boundary.\n",
        "def plot_svc_decision_function(model, ax=None, plot_support=True):\n",
        "    \"\"\"Plot the decision function for a 2D SVC\"\"\"\n",
        "    if ax is None:\n",
        "        ax = plt.gca()\n",
        "    xlim = ax.get_xlim()\n",
        "    ylim = ax.get_ylim()\n",
        "\n",
        "    # create grid to evaluate model\n",
        "    x = np.linspace(xlim[0], xlim[1], 30)\n",
        "    y = np.linspace(ylim[0], ylim[1], 30)\n",
        "    Y, X = np.meshgrid(y, x)\n",
        "    xy = np.vstack([X.ravel(), Y.ravel()]).T\n",
        "    P = model.decision_function(xy).reshape(X.shape)\n",
        "\n",
        "    # plot decision boundary and margins\n",
        "    ax.contour(X, Y, P, colors='r',\n",
        "               levels=[-1, 0, 1], alpha=0.8,\n",
        "               linestyles=['--', '-', '--'])\n",
        "\n",
        "    # plot the support vectors\n",
        "    if plot_support:\n",
        "        ax.scatter(model.support_vectors_[:, 0],\n",
        "                   model.support_vectors_[:, 1],\n",
        "                   s=500, linewidth=10, facecolor='none');\n",
        "    ax.set_xlim(xlim)\n",
        "    ax.set_ylim(ylim)"
      ],
      "metadata": {
        "id": "ufRDRjFfjQ4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the data point distributions:\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=60, cmap='viridis')\n",
        "# Drawing the decision boundaries.\n",
        "plot_svc_decision_function(model);"
      ],
      "metadata": {
        "id": "ASjMY7etjY__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the plot above, there is one solid line (main separating line) and two dashed-lines (margins). Any other lines will have less values of margin than the one above. Some of the training data points in this plot are placed exactly on the margin lines. In other words, margin lines passed through the middle of these points. These important points are called \"Support Vectors\" :"
      ],
      "metadata": {
        "id": "D92bNfbDjgWj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Finding the exact number of Support Vectors.\n",
        "n_support = model.support_vectors_\n",
        "print('The number of support vector is: {}'.format(len(n_support)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zC_YVkfZjmKB",
        "outputId": "527d8538-b92d-4e21-a3dd-9336f2b5de8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of support vector is: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In SVMs, finding the optimum separating line  depends on the position (coordinates) of these Support Vectors. Other points further from the margins are in the correct sides and do not effect the fitting procedure. That is because these points do not contribute to the loss function used to fit the model, so their position and their frequency do not matter as long as they do not cross the margin. We can observe such behavior, for example, if we plot the model learned from the first 70 points and, then, first 140 points of this dataset:"
      ],
      "metadata": {
        "id": "wb25Dm0lj8hC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_svm(N=10, ax=None):\n",
        "    X, y = make_blobs(n_samples=200, centers=2,\n",
        "                      random_state=0, cluster_std=0.60)\n",
        "    X = X[:N]\n",
        "    y = y[:N]\n",
        "    model = SVC(kernel='linear', C=1E10)\n",
        "    model.fit(X, y)\n",
        "\n",
        "    ax = ax or plt.gca()\n",
        "    ax.scatter(X[:, 0], X[:, 1], c=y, s=60, cmap='viridis')\n",
        "    ax.set_xlim(-1, 4)\n",
        "    ax.set_ylim(-1, 6)\n",
        "    plot_svc_decision_function(model, ax)\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
        "fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n",
        "for axi, N in zip(ax, [70, 140]):\n",
        "    plot_svm(N, axi)\n",
        "    axi.set_title('N = {0}'.format(N))"
      ],
      "metadata": {
        "id": "ia0pJwYWjz0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above two figures depict that SVM model fits the line using only Support Vectors and the model is not sensitive to other data points. In fact, the left figure contains half of the data points of the right figure, but both of them have the same separating lines because both of them have similar Support Vectors."
      ],
      "metadata": {
        "id": "GZHhXD1bkKBz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Changing the number of the data points and interpret the results.\n",
        "from ipywidgets import interact, fixed\n",
        "interact(plot_svm, N=[8,10,20,50,100,150,180,300,500,2000], ax=fixed(None));"
      ],
      "metadata": {
        "id": "O_9A3mNKkXVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What about non-linear separable data set?"
      ],
      "metadata": {
        "id": "IkguSSQokxjt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_circles  # Load input data\n",
        "X, y = make_circles(50, factor=.1, noise=0.2)\n",
        "\n",
        "Model1 = SVC(kernel='linear').fit(X, y) # Fit a linear SVM\n",
        "\n",
        "# Plot the data points distribution using the \"plt.scatter\"\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=60, cmap='viridis')\n",
        "\n",
        "# Call the function \"plot_svc_decision_function\" to draw the decision boundaries.\n",
        "plot_svc_decision_function(Model1);\n",
        "\n",
        "# TODO: Run this cell several times (workbook excercise 5.4);"
      ],
      "metadata": {
        "id": "SX9eAprQkqIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Are the drawn lines able to properly separate the classes? why?"
      ],
      "metadata": {
        "id": "VJuuNPwGlS_C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One practical way to handle nonlinear separable data set is to use \"Kernel Trick\". This means we can project our data set to a higher dimension space where the data points are linearly separable. There are different functions for this projection such as Gaussian (Radial) basis function and polynomials. In other words, we map a nonlinear separable data set into a higher dimension space, artificially, then within the new space linear separation would be sufficient."
      ],
      "metadata": {
        "id": "7Ee1174albz0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Run and play with the angles (workbook excercise 5.5)\n",
        "r = np.exp(-(X ** 2).sum(1))\n",
        "from mpl_toolkits import mplot3d\n",
        "\n",
        "def plot_3D(elev=30, azim=30, X=X, y=y):\n",
        "    ax = plt.subplot(projection='3d')\n",
        "    ax.scatter3D(X[:, 0], X[:, 1], r, c=y, s=60, cmap='viridis')\n",
        "    ax.view_init(elev=elev, azim=azim)\n",
        "    ax.set_xlabel('x')\n",
        "    ax.set_ylabel('y')\n",
        "    ax.set_zlabel('r')\n",
        "\n",
        "interact(plot_3D, elev=[-90,-60,0,30,90], azip=(-180,180),\n",
        "         X=fixed(X), y=fixed(y));"
      ],
      "metadata": {
        "id": "qKSzm2StlWU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want to separate the two classes in a higher dimenstion linearly, which line will you use?\n",
        "(workbook 5.6)"
      ],
      "metadata": {
        "id": "uZjZEqp5l2oP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training a Kernel-based SVM"
      ],
      "metadata": {
        "id": "JAT-xyLPl5iF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Model2 = SVC(kernel='rbf', C=1E6) # Creating an SVM with Radial Basis Function\n",
        "Model2.fit(X, y)"
      ],
      "metadata": {
        "id": "OhyTjUgHl45U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the data points distribution using the \"plt.scatter\"\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=60, cmap='viridis')\n",
        "\n",
        "# Call the function \"plot_svc_decision_function\" to draw the decision boundaries.\n",
        "plot_svc_decision_function(Model2)"
      ],
      "metadata": {
        "id": "6JucKX8cmAPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Soft Margin SVMs: So far we have seen data set with a clear decision boundary; however, in real cases data points from different classes often overlapping each other:"
      ],
      "metadata": {
        "id": "KwSrcH1fmMdY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of dataset with class overlapping\n",
        "X, y = make_blobs(n_samples=200, centers=2,random_state=0, cluster_std=1.2)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='viridis');\n",
        "# TODO: Is it possible to use linear or even kernel based SVM to classify these data? (workbook 5.6)"
      ],
      "metadata": {
        "id": "c8my-G0dmIvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In previous examples there were a parameter \"C\" which we set it as a very large number. The role of this parameter is to prevent the data points from appearing inside the margins. The larger this value the lower the chance of data points inside the margins. Accordingly, if we do not allow any points to appear inside the margins we have a \"Hard Margin SVM\" like what we did in previous examples. On the other hand, we can allow the margins to be softer to contain some of the data points."
      ],
      "metadata": {
        "id": "9iugcVrvmUZy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Comparison of soft and hard margin SVM.\n",
        "X, y = make_blobs(n_samples=100, centers=2,random_state=0, cluster_std=0.8) # Input Data\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
        "fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n",
        "\n",
        "for axi, C in zip(ax, [10.0, 0.1]):\n",
        "    model = SVC(kernel='linear', C=C).fit(X, y)\n",
        "    axi.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='viridis')\n",
        "    plot_svc_decision_function(model, axi)\n",
        "    axi.scatter(model.support_vectors_[:, 0],\n",
        "                model.support_vectors_[:, 1],\n",
        "                s=300, lw=1, facecolors='none');\n",
        "    axi.set_title('C = {0:.1f}'.format(C), size=14)\n",
        "\n",
        "# TODO: How do you compare these two figures? (workbook 5.7)"
      ],
      "metadata": {
        "id": "2gGUOo9QmcDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. ###  Random Forrest"
      ],
      "metadata": {
        "id": "mDTX9jxTmk0o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The same ovarian cancer data will be used to train a Random Forest."
      ],
      "metadata": {
        "id": "sjuMiroenIAu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Reading the data (features and labels)\n",
        "data = pd.read_csv('/content/Data1.csv', header=None)\n",
        "x = data.values[:, :100]\n",
        "y = data.values[:, 100]\n",
        "\n",
        "# Splitting the data into train and test\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.2, random_state=0)"
      ],
      "metadata": {
        "id": "xpVwCpK-m84Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compiling a Random Forest model\n",
        "criterion = 'gini' # 'gini' or 'entropy'\n",
        "max_depth = None\n",
        "random_state = 42\n",
        "rf_clf = RandomForestClassifier(criterion = criterion,\n",
        "                            max_depth = max_depth,\n",
        "                            random_state = random_state)"
      ],
      "metadata": {
        "id": "qMRiostPncfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Learning the compiled RF with training data\n",
        "rf_clf.fit(x_train, y_train)\n",
        "# Predicting the labels of unseen data\n",
        "\n",
        "y_pred_rf = rf_clf.predict(x_test)\n",
        "test_acc_rf = accuracy_score(y_test, y_pred_rf)"
      ],
      "metadata": {
        "id": "JPcE8thVoPyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Test accuray of RF model is: {}'.format(test_acc_rf))"
      ],
      "metadata": {
        "id": "pHFrr5Sook0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bmEU7sWcfSPq"
      }
    }
  ]
}