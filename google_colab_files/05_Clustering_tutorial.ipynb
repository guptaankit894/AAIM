{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNbBXEBP+5sRUbvIxLYZBth",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guptaankit894/AAIM/blob/main/google_colab_files/05_Clustering_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data description**Â¶\n",
        "\n",
        "There are 77 expression levels of proteins/protein modifications (columns) that produce detectable signals in the nuclear cortex of Ts65Dn trisomic mice. These proteins are related to a functional hippocampus (learning and memory). In this data there are 38 control mice and 34 trisomic mice (Down syndrome), having 72 mice in total. 15 measurements were carried out for each protein per mouse/sample. In total 570 measurements accounted for control mice and 510 for trisomic mice, having 1080 independent measurements per protein. Some of the values are empty.\n",
        "\n",
        "**Type of data**\n",
        "This data set is multi-levelled as is describing 8 classes of mice in function of genotype, behaviour and treatment. According to genotype, we have control (c) and trisomic (t) mice. If we talk about behaviour, some mice have been stimulated to learn (CS) and others have not (SC). According to treatment, some mice have been treated with the drug memantine (m) to assess the ability of the drug in the learning process and others with saline solution (s). The aim is to identify subsets of proteins that are discriminant between the classes.\n",
        "\n",
        "Classes:\n",
        "c-CS-s: control mice, stimulated to learn, injected with saline (9 mice)\n",
        "c-CS-m: control mice, stimulated to learn, injected with memantine (10 mice)\n",
        "c-SC-s: control mice, not stimulated to learn, injected with saline (9 mice)\n",
        "c-SC-m: control mice, not stimulated to learn, injected with memantine (10 mice)\n",
        "\n",
        "t-CS-s: trisomy mice, stimulated to learn, injected with saline (7 mice)\n",
        "\n",
        "t-CS-m: trisomy mice, stimulated to learn, injected with memantine (9 mice)\n",
        "t-SC-s: trisomy mice, not stimulated to learn, injected with saline (9 mice)\n",
        "t-SC-m: trisomy mice, not stimulated to learn, injected with memantine (9 mice)"
      ],
      "metadata": {
        "id": "-4uEALmW-cJL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing Libraries\n",
        "import pandas as pd # data structure\n",
        "import matplotlib.pyplot as plt # Plotting purpose\n",
        "import numpy as np # Numerical computations\n",
        "import seaborn as sns; sns.set(color_codes=True)  # for plot styling\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage # for dendrogram computation and plotting \"linkage\" function will be used agglomerative clustering\n",
        "from sklearn.decomposition import PCA  # Principal component analysis\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering  # CLustering methods\n",
        "from sklearn import preprocessing #for normalization of features"
      ],
      "metadata": {
        "id": "wCAnZ-90v_hT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xlrd # for reading and formatting data from excel (xls) files"
      ],
      "metadata": {
        "id": "7Q4SzDCFwUb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading data\n",
        "data = pd.read_excel(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00342/Data_Cortex_Nuclear.xls\", header = 0)\n",
        "data.head()"
      ],
      "metadata": {
        "id": "E2VcwfcewVgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Cleaning\n",
        "data = data.dropna()  # Dropping null, and NaNs (Not a number)\n",
        "data = data.drop(['Behavior','Genotype','MouseID','Treatment'], axis=1) # Dropping irrelevant fields from the dataset\n",
        "data.head() # Printing first 5 columns to see the data in the dataframe"
      ],
      "metadata": {
        "id": "0DXVC9tOwZCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample = data.pop('class') # extracting the class colum from the dataset to extract the number of classes\n",
        "\n",
        "#Creating a colour palette for the dendrogram\n",
        "lut = dict(zip(sample.unique(),'bgrcmykw'))\n",
        "row_colors = sample.map(lut)\n",
        "row_colors.head()\n",
        "\n",
        "#Creating a dendrogram with heatmap to visualise data\n",
        "data_a = pd.DataFrame(data)\n",
        "g = sns.clustermap(data_a,row_colors=row_colors, z_score=0,)\n",
        "\n",
        "#Scaling data  to graph a heat map in terms of correlation coefficient\n",
        "data_s = preprocessing.scale(data)\n",
        "data_s = pd.DataFrame(data)\n",
        "\n",
        "#Correlation matrix and heatmap\n",
        "data_s.corr()\n",
        "fig, ax = plt.subplots()\n",
        "fig.set_size_inches(14, 10)\n",
        "ax=sns.heatmap(data_s.corr()) # showing correlation from the heatmap"
      ],
      "metadata": {
        "id": "5UCOJFMXwu3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# finding the suitable number of clusters from the data\n",
        "wcss = [] #  squared distances of the samples\n",
        "for i in range(1, 11):\n",
        "    kmeans = KMeans(n_clusters = i, init = 'k-means++')\n",
        "    #kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)\n",
        "    kmeans.fit(data) # Fitting data for clustering\n",
        "    wcss.append(kmeans.inertia_)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(range(1, 11), wcss)\n",
        "plt.title('The Elbow Method')\n",
        "plt.xlabel('Number of clusters')\n",
        "plt.ylabel('Squared distances of the samples')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "p1QNfdfpyVvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dimensionality Reduction using Principal Component Analysis**"
      ],
      "metadata": {
        "id": "UNjEFzLPGnpp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca=PCA(n_components=2) # just 2 components for simplicity\n",
        "\n",
        "#Fit PCA to the dataset (only variables, excluding class)\n",
        "pca.fit(data)\n",
        "\n",
        "#Calculating rotated PCA scores\n",
        "datatrans=pca.transform(data)\n",
        "classes=sample"
      ],
      "metadata": {
        "id": "5VhWJgT97nrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# K-means clustering for PC components\n",
        "kmeans = KMeans(n_clusters = 7, init = 'k-means++', random_state = 42)\n",
        "pred_kmeans = kmeans.fit_predict(datatrans)\n",
        "#pred_kmeans = pred_kmeans+1\n",
        "\n",
        "plt.figure()\n",
        "sns.scatterplot(x=datatrans[:, 0], y=datatrans[:, 1], hue=pred_kmeans,  palette=\"Set2\", s=80)\n",
        "#plt.scatter(datatrans[:,0],datatrans[:,1],c=pred_kmeans)\n",
        "plt.xlabel('PC1')\n",
        "plt.ylabel('PC2')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RRKojoRTypEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ground truth\n",
        "plt.figure()\n",
        "classes=pd.factorize(classes) # converting classes to numerics\n",
        "sns.scatterplot(x=datatrans[:, 0], y=datatrans[:, 1], hue=classes[0], palette=\"Set2\", s=80)\n",
        "#plt.scatter(datatrans[:,0],datatrans[:,1],c=classes[0])\n",
        "plt.xlabel('PC1')\n",
        "plt.ylabel('PC2')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "smaddKMwyr4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "** Agglomerative Clustering**"
      ],
      "metadata": {
        "id": "KdEHtk9iEMMm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datatrans1=pd.DataFrame(datatrans) # To store the predictions along with the data.\n",
        "datatrans1.head()\n",
        "hierarchical_clustering = AgglomerativeClustering(n_clusters=7)\n",
        "datatrans1[\"Hierarchical_Cluster\"]= hierarchical_clustering.fit_predict(datatrans)"
      ],
      "metadata": {
        "id": "Mo6gqyDi6ead"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#plt.figure(figsize=(10, 5))\n",
        "sns.scatterplot(x=datatrans[:, 0], y=datatrans[:, 1], hue=datatrans1[\"Hierarchical_Cluster\"], palette=\"Set2\", s=80)\n",
        "plt.title(\"Hierarchical Clustering Visualization (PCA Reduced)\")\n",
        "plt.xlabel(\"PCA Component 1\")\n",
        "plt.ylabel(\"PCA Component 2\")\n",
        "plt.legend(title=\"Cluster\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "F3NsQAU6HHty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "linkage_matrix_pca = linkage(datatrans, method='ward')  # Creating a linkage matrix to create a dendrogram\n",
        "plt.figure(figsize=(10, 5))\n",
        "dendrogram(linkage_matrix_pca, truncate_mode='level', p=5, leaf_rotation=90, leaf_font_size=10)\n",
        "plt.title(\"Hierarchical Clustering Dendrogram\")\n",
        "plt.xlabel(\"Sample Index\")\n",
        "plt.ylabel(\"Distance\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DiSetMGh2kI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Performance comparison**"
      ],
      "metadata": {
        "id": "MdPrzUB8J9UW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
        "# Compute Metrics\n",
        "# The higher the better- Higher value means accurate location of points in a particular cluster\n",
        "print(\"K-Means Silhouette Score:\", silhouette_score(datatrans, pred_kmeans))\n",
        "print(\"Agglomerative Silhouette Score:\", silhouette_score(datatrans, datatrans1[\"Hierarchical_Cluster\"].values))\n",
        "\n",
        "# The lower the better- Average similarity between each cluster and its similar cluster\n",
        "print(\"K-Means Davies-Bouldin Index:\", davies_bouldin_score(datatrans, pred_kmeans))\n",
        "print(\"Agglomerative Davies-Bouldin Index:\", davies_bouldin_score(datatrans, datatrans1[\"Hierarchical_Cluster\"].values))\n",
        "\n",
        "# The higher the better-Measures the ratio of between-cluster variance to within-cluster variance.\n",
        "print(\"K-Means Calinski-Harabasz Index:\", calinski_harabasz_score(datatrans, pred_kmeans))\n",
        "print(\"Agglomerative Calinski-Harabasz Index:\", calinski_harabasz_score(datatrans, datatrans1[\"Hierarchical_Cluster\"].values))"
      ],
      "metadata": {
        "id": "on5N3n5_J8iZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}